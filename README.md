# Chatbot---AI-Agent
Sure! Here's a clean, professional, and detailed `README.md` for your project:

---

# ğŸ¤– Personalized AI Chatbot Agent using LangGraph

This project is a **personalized AI agent** built using **LangGraph**, **FastAPI**, and **Streamlit**, with support for **Groq** and **OpenAI** language models. It allows users to define custom agent behavior, interact through a simple UI, and optionally use real-time web search via **Tavily**.

---

## ğŸš€ Features

- ğŸ”§ **Custom System Prompt** â€“ Define your agent's personality and behavior.
- ğŸ§  **Multi-model Support** â€“ Choose from Groq and OpenAI LLMs.
- ğŸŒ **Optional Web Search** â€“ Enable real-time answers using Tavily.
- ğŸ’¬ **Simple Chat UI** â€“ Built with Streamlit for a fast, interactive interface.
- âš¡ **FastAPI Backend** â€“ Handles API communication and model invocation.
- ğŸ§© **LangGraph Agent** â€“ Uses a ReAct-based reasoning agent for advanced responses.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ ai_agent.py          # Core AI agent logic using LangGraph
â”œâ”€â”€ backend.py           # FastAPI backend for handling chat requests
â”œâ”€â”€ frontend.py          # Streamlit-based user interface
â”œâ”€â”€ .env                 # Stores API keys (not committed to version control)
```

---

## ğŸ” Environment Variables

Create a `.env` file in the root directory and add the following:

```env
GROQ_API_KEY=your_groq_api_key
OPENAI_API_KEY=your_openai_api_key
TAVILY_API_KEY=your_tavily_api_key
```

---

## âš™ï¸ Installation

1. **Clone the repo:**
   ```bash
   git clone https://github.com/yourusername/langgraph-ai-agent.git
   cd langgraph-ai-agent
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up `.env` file** with your API keys (see above).

---

## ğŸ–¥ï¸ Running the App

### 1. Start the FastAPI backend:
```bash
python backend.py
```
> This will start the server on `http://127.0.0.1:9999`

---

### 2. Run the Streamlit frontend:
```bash
streamlit run frontend.py
```

---

## ğŸ§  How It Works

- The user enters a system prompt, selects a model, and types a query.
- The frontend sends this input to the FastAPI backend.
- The backend invokes the appropriate LangGraph ReAct agent using:
  - A selected LLM (Groq or OpenAI)
  - A custom system prompt
  - Tavily search tool if enabled
- The final response from the agent is sent back and shown in the UI.

---

## ğŸ§° Tech Stack

- **LangGraph** â€“ For ReAct-style AI agents
- **LangChain** â€“ Model and tool abstraction
- **FastAPI** â€“ Backend API
- **Streamlit** â€“ Frontend UI
- **Tavily** â€“ Optional web search tool
- **Groq / OpenAI** â€“ Language models

---

## ğŸ“Œ Future Improvements

- âœ… Chat history memory
- ğŸ”’ Authentication for agent access
- ğŸ’¾ Agent configuration saving
- ğŸ“„ PDF or document chat support

---

## ğŸ§‘â€ğŸ’» Author

Developed by [Balu Gorad]([https://your-website-or-linkedin.com](https://www.linkedin.com/in/balugorad/))
